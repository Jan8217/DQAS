{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jan8217/DQAS/blob/main/Unitary_learning_replication.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# selecting device for pytorch (either gpu or cpu)\n",
        "is_cuda = True\n",
        "gpuid = 0\n",
        "\n",
        "# selecting float32 for data type (can also be float64)\n",
        "dtype = torch.float64\n",
        "\n",
        "\n",
        "\n",
        "# default grad_enabled\n",
        "grad_enabled_bool = False\n",
        "\n",
        "if is_cuda:\n",
        "\tdevice = torch.device(\"cuda:\"+ str(gpuid))\n",
        "else:\n",
        "\tdevice = torch.device(\"cpu\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hIQSTGxCrA_l"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import gc\n",
        "import numpy as np\n",
        "\n",
        "def create_GUE(n, save_matrix = None):\n",
        "\tmatrix = np.random.normal(0,0.5, [n,n]).astype(np.complex_) + np.array([1j])*np.random.normal(0,0.5, [n,n]).astype(np.complex_)\n",
        "\tdiag_entries = np.random.normal(0,1,[n,1])\n",
        "\tfor i in range(n):\n",
        "\t\tmatrix[i,i] = diag_entries[i]\n",
        "\t# matrix = np.tril(matrix) + np.triu(matrix.T, 1)\n",
        "\tmatrix = ( matrix + np.conjugate(matrix.T) ) / 2\n",
        "\n",
        "\tif save_matrix is not None:\n",
        "\t\twith open(save_matrix, 'wb') as fname:\n",
        "\t\t\tpickle.dump(matrix, fname)\n",
        "\treturn matrix\n",
        "\n",
        "def create_qr_random(n):\n",
        "\tmatrix = np.random.normal(0,1, [n,n]).astype(np.complex_) + np.array([1j])*np.random.normal(0,1, [n,n]).astype(np.complex_)\n",
        "\tmatrix, R = np.linalg.qr(matrix)\n",
        "\tmatrix = (matrix + np.conjugate(matrix.T)) / 2\n",
        "\treturn matrix\n",
        "\n",
        "def load_GUE(save_matrix):\n",
        "\twith open(save_matrix, 'rb') as fname:\n",
        "\t\treturn pickle.load(fname)\n"
      ],
      "metadata": {
        "id": "hCbpdWR0rHyB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\"\"\"\n",
        "Created on Tue Oct  9 06:41:30 2018\n",
        "@author: nsde\n",
        "\"\"\"\n",
        "#%%\n",
        "import torch\n",
        "\n",
        "#%%\n",
        "def torch_expm(A):\n",
        "    \"\"\" \"\"\"\n",
        "    n_A = A.shape[0]\n",
        "    A_fro = torch.sqrt(A.abs().pow(2).sum(dim=(1,2), keepdim=True))\n",
        "\n",
        "    # Scaling step\n",
        "    maxnorm = torch.Tensor([5.371920351148152]).type(A.dtype).to(A.device)\n",
        "    zero = torch.Tensor([0.0]).type(A.dtype).to(A.device)\n",
        "    n_squarings = torch.max(zero, torch.ceil(torch_log2(A_fro / maxnorm)))\n",
        "    Ascaled = A / 2.0**n_squarings\n",
        "    n_squarings = n_squarings.flatten().type(torch.int32)\n",
        "\n",
        "    # Pade 13 approximation\n",
        "    U, V = torch_pade13(Ascaled)\n",
        "    P = U + V\n",
        "    Q = -U + V\n",
        "    R = torch.linalg.solve(Q, P) # solve P = Q*R\n",
        "\n",
        "    # Unsquaring step\n",
        "    expmA = [ ]\n",
        "    for i in range(n_A):\n",
        "        l = [R[i]]\n",
        "        for _ in range(n_squarings[i]):\n",
        "            l.append(l[-1].mm(l[-1]))\n",
        "        expmA.append(l[-1])\n",
        "\n",
        "    return torch.stack(expmA)\n",
        "\n",
        "#%%\n",
        "def torch_log2(x):\n",
        "    return torch.log(x) / torch.log(torch.Tensor([2.0])).type(x.dtype).to(x.device)\n",
        "\n",
        "#%%\n",
        "def torch_pade13(A):\n",
        "    b = torch.Tensor([64764752532480000., 32382376266240000., 7771770303897600.,\n",
        "                      1187353796428800., 129060195264000., 10559470521600.,\n",
        "                      670442572800., 33522128640., 1323241920., 40840800.,\n",
        "                      960960., 16380., 182., 1.]).type(A.dtype).to(A.device)\n",
        "\n",
        "    ident = torch.eye(A.shape[1], dtype=A.dtype).to(A.device)\n",
        "    A2 = torch.matmul(A,A)\n",
        "    A4 = torch.matmul(A2,A2)\n",
        "    A6 = torch.matmul(A4,A2)\n",
        "    U = torch.matmul(A, torch.matmul(A6, b[13]*A6 + b[11]*A4 + b[9]*A2) + b[7]*A6 + b[5]*A4 + b[3]*A2 + b[1]*ident)\n",
        "    V = torch.matmul(A6, b[12]*A6 + b[10]*A4 + b[8]*A2) + b[6]*A6 + b[4]*A4 + b[2]*A2 + b[0]*ident\n",
        "    return U, V\n"
      ],
      "metadata": {
        "id": "VfxZeLbyq3je"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Variable\n",
        "import torch.optim as optim\n",
        "import gc\n",
        "import numpy as np\n",
        "import math\n",
        "import datetime\n",
        "import pandas as pd\n",
        "\n",
        "# directory for saving any matrices or other variables\n",
        "save_dir = './'\n",
        "save_csv = './'\n",
        "\n",
        "class unitary_optimizer():\n",
        "\n",
        "\tdef __init__(self, control_matrices, time_params = None, target = None, random_time_target = None):\n",
        "\t\t'''Initializes a unitary optimizer.\n",
        "\t\tInputs:\n",
        "\t\t\tRequired:\n",
        "\t\t\t\tcontrol_matrices: batched list of pytorch matrices multiplied in order\n",
        "\t\t\tOne of the following:\n",
        "\t\t\t\ttime_params: batched list of time vectors for the control matrices to construct the target\n",
        "\t\t\t\ttarget: target matrix in pytorch format\n",
        "\t\t\t\trandom_time_target = integer of number of random multiplications of control matrices to create target\n",
        "\t\t'''\n",
        "\t\tsuper(unitary_optimizer, self).__init__()\n",
        "\n",
        "\t\tself.manual_grad_calc = False # default to False, can be set to True during optimize phase\n",
        "\n",
        "\t\tif len(control_matrices.size()) == 4:\n",
        "\t\t\tself.control_matrices_real = complex_matrix_setup(control_matrices)\n",
        "\t\t\tself.control_matrices = control_matrices\n",
        "\t\telse:\n",
        "\t\t\tself.control_matrices_real = control_matrices\n",
        "\t\t\tself.control_matrices = convert_to_4d_batch(control_matrices)\n",
        "\n",
        "\t\tself.n_control_matrices = self.control_matrices.size()[0]\n",
        "\t\tself.dim_matrix = self.control_matrices_real.size()[1]\n",
        "\n",
        "\t\tif target is not None:\n",
        "\t\t\tself.target = target\n",
        "\t\t\tif len(self.target.size()) == 2:\n",
        "\t\t\t\tself.target = self.target.reshape([1]+list(self.target.size()))\n",
        "\t\t\tif len(self.target.size()) == 4:\n",
        "\t\t\t\tself.target = complex_matrix_setup(self.target)\n",
        "\t\t\tself.n_target_times = -1\n",
        "\t\telif time_params is not None:\n",
        "\t\t\ttime_params = self.format_time_tensor(time_params, grad_bool = True)\n",
        "\t\t\tself.target = self.construct_matrix(time_params)\n",
        "\t\t\tself.n_target_times = time_params.size()[1]\n",
        "\t\telif random_time_target is not None:\n",
        "\t\t\tself.target = self.construct_random_matrix(random_time_target)\n",
        "\t\t\tself.n_target_times = random_time_target\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('either target, random_time_target, or time_params must be inputted when initializing unitary_optimizer')\n",
        "\n",
        "\t\tself.output = real_identity(self.target.size())\n",
        "\n",
        "\n",
        "\n",
        "\tdef full_loss_calc(self):\n",
        "\t\tself.output = self.construct_matrix(self.times)\n",
        "\t\tloss = frobenius_norm(self.output, self.target)\n",
        "\t\treturn loss\n",
        "\n",
        "\tdef format_time_tensor(self, time_tensor, grad_bool = True):\n",
        "\t\tdepth = time_tensor.size()[1]\n",
        "\t\ttimes_tiled = time_tensor.t().reshape((time_tensor.size()[0]*depth, 1))\n",
        "\t\treturn torch.tensor(times_tiled, device = device, dtype = dtype, requires_grad = grad_bool)\n",
        "\n",
        "\tdef construct_matrix(self, time_params, multiply_forward = True, forward_pass = True):\n",
        "\t\tif self.manual_grad_calc:\n",
        "\t\t\tself.matrix_exp, self.imaginary_matrices = batch_matrix_exp(self.control_matrices_real, time_params, return_imaginary_matrices = True)\n",
        "\t\telse:\n",
        "\t\t\tself.matrix_exp = batch_matrix_exp(self.control_matrices_real, time_params)\n",
        "\t\tif self.manual_grad_calc and forward_pass:\n",
        "\t\t\tout_mat = self.ordered_matrix_multiply(self.matrix_exp, multiply_forward = multiply_forward)\n",
        "\t\telse:\n",
        "\t\t\tout_mat = batch_matrix_multiply(self.matrix_exp)\n",
        "\t\treturn out_mat\n",
        "\n",
        "\tdef construct_random_matrix(self, n_times = 10, grad_bool = True):\n",
        "\t\ttimes = self.randomly_initialize_times(n_times, grad_bool = grad_bool)\n",
        "\t\treturn self.construct_matrix(times)\n",
        "\n",
        "\tdef randomly_initialize_times(self, n_times, grad_bool = True, uniform_range = 2.):\n",
        "\t\trand_times = ( torch.rand( (self.n_control_matrices, n_times),\n",
        "\t\t\tdevice = device, dtype = dtype) - 0.5 ) * 2*uniform_range\n",
        "\t\treturn self.format_time_tensor(rand_times, grad_bool = grad_bool)\n",
        "\n",
        "\tdef greedy_optimize(self, max_n_times = 100, epochs_per = 1000, print_statistics = True,\n",
        "\t\tmanual_grad_calc = True, init_type = 'zeros', optim_type = 'Adam', relative_stop_rate = 1e-8,\n",
        "\t\tabsolute_stop_rate = 1e-7,\tprint_every = 100, min_epochs = 15, init_range = 0.01, normalize_learning_rate = 1.0,\n",
        "\t\t **kwargs):\n",
        "\t\t'''optimizes by adding a layer of times each round and optimizing until convergence\n",
        "\n",
        "\t\tInputs:\n",
        "\t\t\tmax_n_times: (int) maximum depth (e.g. 10 means there will be 10 parameters per control matrix)\n",
        "\t\t\tepochs_per: (int) epochs of training per round\n",
        "\t\t\tprint_statistics: (bool) prints error values if set to True\n",
        "\t\t\tprint_every: (int) print error values every given iteration\n",
        "\t\t\tmanual_grad_calc: (bool) determines whether pytorch automatically calculates gradients or manually calculated (set to True for now)\n",
        "\t\t\tinit_type: (string) choose from 'zeros', 'random', or 'search'\n",
        "\t\t\toptim_type: (string) choose 'Adam', 'SGD', or 'LBFGS' for optimization method\n",
        "\t\t\trelative_stop_rate: (double) value of relative stop rate to move onto next round\n",
        "\t\t\tabsolute_stop_rate: (double) value to stop after convergence reached\n",
        "\t\t\t**kwargs: arguments passed into the chosen pytorch optimizer\n",
        "\t\t'''\n",
        "\n",
        "\t\t# set values for inputs\n",
        "\t\tself.relative_stop_rate = relative_stop_rate\n",
        "\t\tself.absolute_stop_rate = absolute_stop_rate\n",
        "\t\tself.max_epochs = epochs_per\n",
        "\t\tself.min_epochs = min_epochs\n",
        "\n",
        "\t\tself.optim_type = optim_type\n",
        "\n",
        "\t\tif init_type == 'random':\n",
        "\t\t\tself.times = self.randomly_initialize_times(1, grad_bool = True)\n",
        "\t\telif init_type == 'search':\n",
        "\t\t\tself.times = self.search_times_append(grid_range = init_range)\n",
        "\t\telif init_type == 'zeros':\n",
        "\t\t\tinit_times = torch.zeros([self.n_control_matrices, 1], device = device, dtype = dtype)\n",
        "\t\t\tself.times = self.format_time_tensor(init_times)\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('Please input valid initialization type')\n",
        "\n",
        "\t\tself.n_times = 1\n",
        "\n",
        "\t\tself.manual_grad_calc = manual_grad_calc\n",
        "\n",
        "\t\tfor round_i in range(max_n_times):\n",
        "\t\t\tprint()\n",
        "\t\t\tprint('Beginning round {}'.format(round_i+1))\n",
        "\t\t\tif print_statistics:\n",
        "\t\t\t\tprint('Starting times for round:')\n",
        "\t\t\t\tprint(self.times)\n",
        "\n",
        "\t\t\tkwargs['lr'] = kwargs['lr']/ normalize_learning_rate\n",
        "\n",
        "\t\t\tif optim_type == 'Adam':\n",
        "\t\t\t\tself.optimizer = optim.Adam([self.times], **kwargs)\n",
        "\t\t\telif optim_type == 'SGD':\n",
        "\t\t\t\tself.optimizer = optim.SGD([self.times], **kwargs)\n",
        "\t\t\telif optim_type == 'LBFGS':\n",
        "\t\t\t\tself.optimizer = optim.LBFGS([self.times], **kwargs)\n",
        "\t\t\telse:\n",
        "\t\t\t\traise ValueError('Please input valid optimization type')\n",
        "\n",
        "\t\t\tloss = self.optimize_round(print_statistics = True, print_every = print_every)\n",
        "\n",
        "\t\t\tif loss < self.absolute_stop_rate:\n",
        "\t\t\t\tprint('Final loss: {}'.format(loss))\n",
        "\t\t\t\tprint('Times:')\n",
        "\t\t\t\tprint(self.times)\n",
        "\t\t\t\treturn round_i\n",
        "\n",
        "\t\t\tprint('Loss at end of round: {}'.format(loss))\n",
        "\t\t\tif print_statistics:\n",
        "\t\t\t\tprint('New Times:')\n",
        "\t\t\t\tprint(self.times)\n",
        "\n",
        "\t\t\t# if not achieve absolute_stop_rate, append new times\n",
        "\t\t\tself.append_times(init_type, init_range)\n",
        "\t\treturn -1\n",
        "\n",
        "\n",
        "\tdef append_times(self, init_type, init_range):\n",
        "\t\t'''method used in greedy_optimize to append time to self.times\n",
        "\n",
        "\t\tInputs:\n",
        "\t\t\tinit_type: (string) set to given strings to specify type of initialization\n",
        "\t\t\tinit_range: (double) range of initialization for randomization or search\n",
        "\t\t'''\n",
        "\t\tif init_type == 'random':\n",
        "\t\t\tnew_times = self.randomly_initialize_times(1, grad_bool = True, uniform_range = init_range)\n",
        "\t\telif init_type == 'search':\n",
        "\t\t\tnew_times = self.search_times_append(grid_range = init_range)\n",
        "\t\telif init_type == 'zeros':\n",
        "\t\t\tinit_times = torch.zeros([self.n_control_matrices, 1], device = device, dtype = dtype)\n",
        "\t\t\tnew_times = self.format_time_tensor(init_times)\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('Please input valid initialization type')\n",
        "\n",
        "\t\tself.times = torch.cat( (self.times, new_times) )\n",
        "\t\tself.n_times += 1\n",
        "\n",
        "\n",
        "\tdef optimize_round(self, print_statistics = True, print_every = 10):\n",
        "\t\t''' method used in greedy_optimize to optimize each round\n",
        "\t\t'''\n",
        "\t\t# intialize epoch number and loss values\n",
        "\t\tepoch_i = 0\n",
        "\t\tchange_loss = self.relative_stop_rate*10 # initialize to something larger than the relative_stop rate\n",
        "\t\tprior_loss = 99999999\n",
        "\n",
        "\t\twhile (epoch_i < self.max_epochs and change_loss > self.relative_stop_rate) \\\n",
        "\t\tor epoch_i < self.min_epochs:\n",
        "\t\t\tif not self.manual_grad_calc:\n",
        "\t\t\t\tself.optimizer.zero_grad()\n",
        "\n",
        "\t\t\tloss = self.full_loss_calc()\n",
        "\n",
        "\t\t\tif print_statistics:\n",
        "\t\t\t\tif epoch_i%print_every == 0:\n",
        "\t\t\t\t\tprint('[%d] loss: %.3E' % (epoch_i , loss ))\n",
        "\n",
        "\t\t\tif self.manual_grad_calc:\n",
        "\t\t\t\tself.times.grad = self.manual_gradients()\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss.backward()\n",
        "\n",
        "\t\t\tif self.optim_type == 'LBFGS':\n",
        "\t\t\t\tself.optimizer.step(self.full_loss_calc)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.optimizer.step()\n",
        "\n",
        "\t\t\tchange_loss = (prior_loss - loss) / prior_loss\n",
        "\t\t\tprior_loss = loss\n",
        "\t\t\tepoch_i += 1\n",
        "\n",
        "\t\tif print_statistics:\n",
        "\t\t\tprint('Total number of epochs: {}'.format(epoch_i))\n",
        "\n",
        "\t\treturn loss\n",
        "\n",
        "\n",
        "\tdef search_times_append(self, n_grid = 1001, grid_range = math.pi, appending = True):\n",
        "\t\t# create grid of times\n",
        "\t\tgrid_times = torch.linspace(-1*grid_range,grid_range,steps=n_grid, device = device, dtype = dtype)\n",
        "\t\tgrid_times = torch.reshape(grid_times, [n_grid,1])\n",
        "\n",
        "\t\t# initialize vector containing new times\n",
        "\t\tnew_times = torch.zeros([self.n_control_matrices, 1], device = device, dtype = dtype)\n",
        "\n",
        "\t\tfor i, matrix_i in enumerate(self.control_matrices_real):\n",
        "\t\t\t# find optimal new time and matrix\n",
        "\t\t\toptim_time, optim_matrix = self.grid_loss_search(torch.reshape(matrix_i, [1,self.dim_matrix, self.dim_matrix]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t grid_times, appending = appending)\n",
        "\t\t\tnew_times[i] = optim_time\n",
        "\t\t\t# update output to take into account change\n",
        "\t\t\tself.output = torch.reshape(optim_matrix, [1,self.dim_matrix, self.dim_matrix])\n",
        "\n",
        "\t\treturn new_times\n",
        "\n",
        "\tdef propogate_round(self, n_grid = 1001, grid_range = math.pi, appending = False):\n",
        "\n",
        "\t\t# getting a backwards pass\n",
        "\t\tself.construct_matrix(self.times, multiply_forward = False)\n",
        "\n",
        "\t\t# start back_matrix at identity\n",
        "\t\tforward_matrix = torch.eye(self.dim_matrix, dtype = dtype, device = device).reshape(-1,self.dim_matrix, self.dim_matrix)\n",
        "\n",
        "\t\t# loop backwards in forward pass (start from second to last matrix being multiplied)\n",
        "\t\tfor i, back_matrix_i in enumerate(self.backward_pass):\n",
        "\t\t\t# print(i)\n",
        "\t\t\t# print('back_matrix')\n",
        "\t\t\t# print(back_matrix_i)\n",
        "\t\t\t# print('forward_matrix')\n",
        "\t\t\t# print(forward_matrix)\n",
        "\n",
        "\t\t\t# create grid of times\n",
        "\t\t\tgrid_times = self.times[i] + torch.linspace(-1*grid_range,grid_range,steps=n_grid, device = device, dtype = dtype)\n",
        "\t\t\tgrid_times = torch.reshape(grid_times, [n_grid,1])\n",
        "\t\t\t# print(grid_times)\n",
        "\n",
        "\t\t\tmatrix_i = self.control_matrices_real[i%self.n_control_matrices]\n",
        "\t\t\t# print('matrix')\n",
        "\t\t\t# print(matrix_i)\n",
        "\n",
        "\n",
        "\t\t\t# find optimal new time and matrix\n",
        "\t\t\toptim_time, optim_matrix, optim_exp = self.grid_loss_search(torch.reshape(matrix_i, [1,self.dim_matrix, self.dim_matrix]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t grid_times, appending = appending,\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t back_matrix = torch.reshape(back_matrix_i, [1,self.dim_matrix, self.dim_matrix]),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t forward_matrix = torch.reshape(forward_matrix, [1,self.dim_matrix, self.dim_matrix])\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t )\n",
        "\t\t\t# update output to take into account change\n",
        "\t\t\tself.output = torch.reshape(optim_matrix, [1,self.dim_matrix, self.dim_matrix])\n",
        "\n",
        "\t\t\t# update time\n",
        "\t\t\tself.times[i] = optim_time\n",
        "\n",
        "\t\t\t# update forward_matrix\n",
        "\t\t\tforward_matrix = torch.matmul(optim_exp ,forward_matrix)\n",
        "\t\t\t# print(self.output)\n",
        "\n",
        "\t\t\t# print(self.full_loss_calc())\n",
        "\n",
        "\t\t# return new_times\n",
        "\n",
        "\tdef grid_loss_search(self, matrix, times, appending = True, back_matrix = None, forward_matrix = None):\n",
        "\t\t'''performs matrix multiplications on matrix and determines which has lowest loss\n",
        "\t\t'''\n",
        "\t\t# print(back_matrix)\n",
        "\n",
        "\n",
        "\t\tmatrix = batch_matrix_exp(matrix, times)\n",
        "\t\tif appending:\n",
        "\t\t\tnew_outputs = torch.matmul(matrix,self.output.expand(matrix.size()))\n",
        "\t\telse:\n",
        "\t\t\tnew_outputs = torch.matmul(matrix, forward_matrix.expand(matrix.size()))\n",
        "\t\t\tnew_outputs = torch.matmul(back_matrix.expand(matrix.size()), new_outputs)\n",
        "\n",
        "\t\tloss_values = batch_frobenius_norm(new_outputs, self.target.expand(new_outputs.size()))\n",
        "\t\tmin_ind = torch.argmin(loss_values)\n",
        "\t\t# print(loss_values)\n",
        "\t\t# print(loss_values[min_ind])\n",
        "\t\t# print(matrix[min_ind])\n",
        "\n",
        "\t\t# print(forward_matrix)\n",
        "\t\tif appending:\n",
        "\t\t\treturn times[min_ind], new_outputs[min_ind]\n",
        "\t\telse:\n",
        "\t\t\treturn times[min_ind], new_outputs[min_ind], matrix[min_ind]\n",
        "\n",
        "\n",
        "\tdef optimize(self, n_times = 2, n_epochs = 2000, print_statistics = True,\n",
        "\t\tmanual_grad_calc = True, init_times = None, optim_type = 'Adam',\n",
        "\t\tprint_every = 100, save_results = False, absolute_stop_rate = 1e-7,\n",
        "\t\tinclude_propogation = False, track_times = False,\n",
        "\t\tpropogate_every = 100, propogate_n_grid = 1001, propogate_grid_range = 5.0,\n",
        "\t\t**kwargs):\n",
        "\t\t'''\n",
        "\t\tInputs:\n",
        "\t\t\t**kwargs = variable list of arguments for the pytorch adam optimizer\n",
        "\t\t'''\n",
        "\n",
        "\t\tif save_results:\n",
        "\t\t\tloss_tracking = []\n",
        "\t\t\tepoch_list = []\n",
        "\t\t\tl2_dist_list = []\n",
        "\t\t\tl1_dist_list = []\n",
        "\n",
        "\t\tif track_times:\n",
        "\t\t\tself.time_tracker = []\n",
        "\n",
        "\t\tif init_times is None:\n",
        "\t\t\tself.times = self.randomly_initialize_times(n_times, grad_bool = True)\n",
        "\t\t\tself.n_times = n_times\n",
        "\t\telse:\n",
        "\t\t\tself.n_times = int(init_times.size()[1])\n",
        "\t\t\tself.times = self.format_time_tensor(init_times)\n",
        "\n",
        "\t\tself.init_times = self.times.clone().detach()\n",
        "\n",
        "\t\tif optim_type == 'Adam':\n",
        "\t\t\tself.optimizer = optim.Adam([self.times], **kwargs)\n",
        "\t\telif optim_type == 'SGD':\n",
        "\t\t\tself.optimizer = optim.SGD([self.times], **kwargs)\n",
        "\t\telif optim_type == 'LBFGS':\n",
        "\t\t\tself.optimizer = optim.LBFGS([self.times], **kwargs)\n",
        "\t\telse:\n",
        "\t\t\traise ValueError('Please input valid optimization type')\n",
        "\t\tself.manual_grad_calc = manual_grad_calc\n",
        "\n",
        "\t\tfor epoch_i in range(n_epochs):\n",
        "\t\t\tif include_propogation:\n",
        "\t\t\t\tif epoch_i%propogate_every == 0:\n",
        "\t\t\t\t\tprint('propogating at epoch {}'.format(epoch_i))\n",
        "\t\t\t\t\tself.propogate_round(n_grid = propogate_n_grid,\n",
        "\t\t\t\t\t\t\t\t\t\t grid_range = propogate_grid_range,\n",
        "\t\t\t\t\t\t\t\t\t\t appending = False)\n",
        "\n",
        "\n",
        "\t\t\tif not self.manual_grad_calc:\n",
        "\t\t\t\tself.optimizer.zero_grad()\n",
        "\n",
        "\t\t\tloss = self.full_loss_calc()\n",
        "\t\t\tif print_statistics:\n",
        "\t\t\t\tif epoch_i%print_every == 0:\n",
        "\t\t\t\t\tprint('[%d] loss: %.3E' % (epoch_i + 1, loss ))\n",
        "\t\t\t\t\tif save_results:\n",
        "\t\t\t\t\t\tepoch_list.append(epoch_i)\n",
        "\t\t\t\t\t\tloss_tracking.append(loss.data.cpu().numpy())\n",
        "\n",
        "\t\t\t\t\t\tdiff_times = self.times - self.init_times\n",
        "\t\t\t\t\t\tl1_norm = torch.sum( torch.abs(diff_times) )\n",
        "\t\t\t\t\t\tl1_dist_list.append(l1_norm.data.cpu().numpy())\n",
        "\t\t\t\t\t\tl2_norm = torch.sqrt( torch.sum( diff_times*diff_times ) )\n",
        "\t\t\t\t\t\tl2_dist_list.append(l2_norm.data.cpu().numpy())\n",
        "\n",
        "\t\t\tif manual_grad_calc:\n",
        "\t\t\t\tself.times.grad = self.manual_gradients()\n",
        "\t\t\telse:\n",
        "\t\t\t\tloss.backward()\n",
        "\n",
        "\t\t\tif loss < absolute_stop_rate:\n",
        "\t\t\t\tprint('Final loss: {}'.format(loss))\n",
        "\t\t\t\tbreak\n",
        "\n",
        "\t\t\tif optim_type == 'LBFGS':\n",
        "\t\t\t\tself.optimizer.step(self.full_loss_calc)\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.optimizer.step()\n",
        "\n",
        "\t\t\tif track_times:\n",
        "\t\t\t\tself.time_tracker.append(self.times.clone().detach())\n",
        "\n",
        "\n",
        "\n",
        "\t\tif save_results:\n",
        "\t\t\tepoch_list.append(epoch_i+1)\n",
        "\t\t\tloss_tracking.append(loss.data.cpu().numpy())\n",
        "\t\t\tdiff_times = self.times - self.init_times\n",
        "\t\t\tl1_norm = torch.sum( torch.abs(diff_times) )\n",
        "\t\t\tl1_dist_list.append(l1_norm.data.cpu().numpy())\n",
        "\t\t\tl2_norm = torch.sqrt( torch.sum( diff_times*diff_times ) )\n",
        "\t\t\tl2_dist_list.append(l2_norm.data.cpu().numpy())\n",
        "\n",
        "\t\t\tpd_cols = {'number of time parameters': n_times*self.n_control_matrices,\n",
        "\t\t\t\t'dimension of unitary matrix': self.dim_matrix/2,\n",
        "\t\t\t\t'number of target parameters': self.n_target_times*self.n_control_matrices,\n",
        "\t\t\t\t'gradient descent step': epoch_list,\n",
        "\t\t\t\t'l1 distance traversed': l1_dist_list,\n",
        "\t\t\t\t'l2 distance traversed': l2_dist_list,\n",
        "\t\t\t\t'loss': loss_tracking\n",
        "\t\t\t\t}\n",
        "\n",
        "\t\t\tdf = pd.DataFrame(data=pd_cols)\n",
        "\t\t\tcurrentDT = datetime.datetime.now()\n",
        "\t\t\tdf.to_csv(save_csv+str(currentDT)+'.csv')\n",
        "\t\t\tprint(df)\n",
        "\n",
        "\n",
        "\tdef manual_gradients(self):\n",
        "\t\t''' manaully calculates gradients using backwards pass\n",
        "\t\t'''\n",
        "\t\t# initialize gradients\n",
        "\n",
        "\t\tself.times.grad = torch.zeros([self.n_times*self.n_control_matrices, 1],\n",
        "\t\t\t\t\tdevice = device, dtype = dtype)\n",
        "\n",
        "\t\tself.ordered_matrix_multiply(self.matrix_exp, multiply_forward = False)\n",
        "\n",
        "\t\tgrad_mats = torch.matmul( self.backward_pass, torch.matmul(self.imaginary_matrices,self.forward_pass) )\n",
        "\t\tgrad_mats = torch.matmul(torch.transpose(self.target, 1, 2).expand(grad_mats.size()), grad_mats)\n",
        "\t\tgrads = -1*torch.sum(torch.diagonal(grad_mats, dim1 = 1, dim2 = 2), 1)\n",
        "\t\treturn grads.reshape([len(grads),1])\n",
        "\n",
        "\n",
        "\tdef ordered_matrix_multiply(self, matrices_in, multiply_forward = True):\n",
        "\t\tmatrices = torch.clone(matrices_in)\n",
        "\t\tn_mats = matrices.size()[0]\n",
        "\t\tdim_mat = matrices.size()[1]\n",
        "\t\tif multiply_forward:\n",
        "\t\t\tfor i in range(n_mats - 1):\n",
        "\t\t\t\tmatrices[i+1] = torch.matmul(matrices[i+1], matrices[i])\n",
        "\t\telse:\n",
        "\t\t\tshifted_matrices = torch.zeros(matrices.size(), device = device, dtype = dtype)\n",
        "\t\t\tshifted_matrices[-1,:,:] = torch.eye(matrices.size()[1], device = device, dtype = dtype)\n",
        "\t\t\tfor i in range(n_mats - 1):\n",
        "\t\t\t\tshifted_matrices[-(i+2)] = torch.matmul(shifted_matrices[-(i+1)], matrices[-(i+1)])\n",
        "\n",
        "\n",
        "\t\tif self.manual_grad_calc:\n",
        "\t\t\tif multiply_forward:\n",
        "\t\t\t\tself.forward_pass = matrices\n",
        "\t\t\telse:\n",
        "\t\t\t\tself.backward_pass = shifted_matrices\n",
        "\t\t\t\treturn None \t\t\t\t\t# no return if backwards pass\n",
        "\n",
        "\t\treturn matrices[-1].view([1] + list(matrices.size()[1:]))\n",
        "\n",
        "\n",
        "\tdef get_loss_grid(self,\n",
        "\t\t\t\t\t  direction1 = None, direction2 = None, start_point = None,\n",
        "\t\t\t\t\t  n_grid_steps = 5, grid_size = 0.25):\n",
        "\t\t'''returns 2d grid of loss values\n",
        "\t\tuseful for plotting the loss function as a contour\n",
        "\t\t'''\n",
        "\n",
        "\t\tif direction1 is None:\n",
        "\t\t\tdirection1 = torch.randn(*self.times.size(),\n",
        "\t\t\t\tdevice = device, dtype = dtype)\n",
        "\n",
        "\t\tif direction2 is None:\n",
        "\t\t\tdirection2 = torch.randn(*self.times.size(),\n",
        "\t\t\t\tdevice = device, dtype = dtype)\n",
        "\n",
        "\t\tif start_point is None:\n",
        "\t\t\tstart_point = self.times.clone().detach()\n",
        "\n",
        "\n",
        "\t\t# normalizing directions\n",
        "\t\tl1 = torch.sqrt(torch.sum( direction1**2 ))\n",
        "\t\tl2 = torch.sqrt(torch.sum( direction2**2 ))\n",
        "\t\td1 = direction1 / l1\n",
        "\t\td2 = direction2 / l2\n",
        "\n",
        "\t\t# get points in grid\n",
        "\t\tx_points = torch.linspace(-1*grid_size, grid_size, steps = n_grid_steps )\n",
        "\t\ty_points = torch.linspace(-1*grid_size, grid_size, steps = n_grid_steps )\n",
        "\n",
        "\t\t# initialize essentials\n",
        "\t\tx_vals = []\n",
        "\t\ty_vals = []\n",
        "\t\tloss_vals = []\n",
        "\t\ttemp_store_times = self.times.clone().detach()\n",
        "\n",
        "\t\t# loop through and calculate loss\n",
        "\t\tfor x in x_points:\n",
        "\t\t\tprint(x)\n",
        "\t\t\tfor y in y_points:\n",
        "\t\t\t\tself.times = start_point + x*d1 + y*d2\n",
        "\t\t\t\tloss = self.full_loss_calc()\n",
        "\t\t\t\tx_vals.append(float(x.data.cpu()))\n",
        "\t\t\t\ty_vals.append(float(y.data.cpu()))\n",
        "\t\t\t\tloss_vals.append(float(loss.data.cpu()))\n",
        "\n",
        "\t\tself.times = temp_store_times\n",
        "\t\treturn x_vals, y_vals, loss_vals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def batch_matrix_multiply(matrices):\n",
        "\tn_mats = matrices.size()[0]\n",
        "\t# if only one matrix left, recursion is done\n",
        "\tif n_mats == 1:\n",
        "\t\treturn matrices\n",
        "\t# if odd number, multiply last matrices to get even number\n",
        "\tif n_mats % 2 == 1:\n",
        "\t\tmatrices[1,:,:] = torch.matmul(matrices[1,:,:], matrices[0,:,:])\n",
        "\t\tmatrices = matrices[1:, :, :]\n",
        "\t\tn_mats = n_mats-1\n",
        "\n",
        "\teven_mats = list(range(0,n_mats,2))\n",
        "\todd_mats = [i+1 for i in even_mats]\n",
        "\tmatrices = torch.matmul(matrices[odd_mats], matrices[even_mats])\n",
        "\treturn batch_matrix_multiply(matrices)\n",
        "\n",
        "def batch_matrix_exp(matrices, times, is_pauli = False, return_imaginary_matrices = False):\n",
        "\tdepth = int(times.size()[0]/matrices.size()[0])\n",
        "\n",
        "\tif is_pauli:\n",
        "\t\texpanded_matrices = matrices.repeat([depth, 1, 1])\n",
        "\t\timaginary_matrices = torch.matmul(imaginary_identity( expanded_matrices.size() ), expanded_matrices )\n",
        "\t\texpanded_matrices = batch_matrix_scalar(torch.cos(times), real_identity( expanded_matrices.size() )) + \\\n",
        "\t\t\t\t\t\t\tbatch_matrix_scalar(torch.sin(times), imaginary_matrices )\n",
        "\telse:\n",
        "\t\timaginary_matrices = torch.matmul(imaginary_identity( matrices.size() ), matrices )\n",
        "\t\timaginary_matrices = imaginary_matrices.repeat([depth, 1, 1])\n",
        "\t\texpanded_matrices = batch_matrix_scalar(times, imaginary_matrices)\n",
        "\t\texpanded_matrices = torch_expm(expanded_matrices)\n",
        "\n",
        "\tif return_imaginary_matrices:\n",
        "\t\treturn [expanded_matrices, imaginary_matrices]\n",
        "\telse:\n",
        "\t\treturn expanded_matrices\n",
        "\n",
        "def batch_matrix_scalar(scalars, matrices):\n",
        "\tn, r, c = list(matrices.size())\n",
        "\treturn (matrices.view((n,r*c))*scalars).view((n,r,c))\n",
        "\n",
        "def complex_matrix_setup(batch_in):\n",
        "\t'''Converts nx2xdxd matrix to a nx2*dx2*d matrix to compute imaginary\n",
        "\tmatrix calculations using real numbers\n",
        "\t'''\n",
        "\tsize_in = batch_in.size()\n",
        "\tn = size_in[0]\n",
        "\td = size_in[2]\n",
        "\n",
        "\t# initialize output matrix\n",
        "\tbatch_out = torch.zeros( (n, d*2, d*2), dtype = dtype, device = device )\n",
        "\n",
        "\t# first setup real numbers\n",
        "\tbatch_out[:, :d, :d] = batch_in[:,0,:,:]\n",
        "\tbatch_out[:, d:, d:] = batch_in[:,0,:,:]\n",
        "\n",
        "\t# next, setup imaginary numbers\n",
        "\tbatch_out[:, :d, d:] = batch_in[:,1,:,:] * (-1)\n",
        "\tbatch_out[:, d:, :d] = batch_in[:,1,:,:]\n",
        "\n",
        "\treturn batch_out\n",
        "\n",
        "\n",
        "def convert_to_4d_batch(batch_in):\n",
        "\t'''Converts nx2*dx2*d matrix to nx2xdxd matrix to simplify viewing matrices\n",
        "\t'''\n",
        "\tsize_in = batch_in.size()\n",
        "\tn = int(size_in[0])\n",
        "\td = int(size_in[1]/2)\n",
        "\n",
        "\t# initialize output matrix\n",
        "\tbatch_out = torch.zeros( (n, 2, d, d), dtype = dtype, device = device )\n",
        "\n",
        "\t# first setup real numbers\n",
        "\tbatch_out[:,0,:,:] = batch_in[:,:d,:d]\n",
        "\n",
        "\t# next, setup imaginary numbers\n",
        "\tbatch_out[:,1,:,:] = batch_in[:, d:, :d]\n",
        "\n",
        "\treturn batch_out\n",
        "\n",
        "def frobenius_norm(A,B):\n",
        "\t''' Calculates Frobenius Norm of A-B, assuming format of complex structured matrix\n",
        "\t'''\n",
        "\tmat_norm = torch.matmul(torch.transpose(A, 1, 2).expand(B.size()), B)\n",
        "\ttrace = torch.sum(torch.diagonal(mat_norm, dim1 = 1, dim2 = 2), 1)\n",
        "\n",
        "\treturn mat_norm.size()[1]*trace.size()[0] - torch.sum(trace)\n",
        "\n",
        "def batch_frobenius_norm(A,B):\n",
        "\t''' Calculates Frobenius Norm of A-B, assuming format of complex structured matrix\n",
        "\tPerformed in batches where multiple As and Bs can be inputted\n",
        "\t'''\n",
        "\tmat_norm = torch.matmul(torch.transpose(A, 1, 2).expand(B.size()), B)\n",
        "\ttrace = torch.sum(torch.diagonal(mat_norm, dim1 = 1, dim2 = 2), 1)\n",
        "\n",
        "\treturn mat_norm.size()[1] - trace\n",
        "\n",
        "\n",
        "def imaginary_identity(shape_out):\n",
        "\t'''Creates identity matrix that has i as value on all diagonal entries\n",
        "\t'''\n",
        "\tshape_out = list(shape_out)\n",
        "\timaginary_id = torch.zeros([1]+shape_out[1:], dtype = dtype, device = device)\n",
        "\thalf_point = int(shape_out[1]/2)\n",
        "\timaginary_id[:, :half_point, half_point:] = -torch.eye(half_point, dtype = dtype, device = device)\n",
        "\timaginary_id[:, half_point:, :half_point] = torch.eye(half_point, dtype = dtype, device = device)\n",
        "\treturn imaginary_id.expand(shape_out)\n",
        "\n",
        "def real_identity(shape_out):\n",
        "\t'''Create batched identity matrix\n",
        "\t'''\n",
        "\tshape_out = list(shape_out)\n",
        "\treal_id = torch.eye(shape_out[1], dtype = dtype, device = device).view([1]+shape_out[1:])\n",
        "\treturn real_id.expand(shape_out)"
      ],
      "metadata": {
        "id": "yT3S58l4q9JJ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H5B0wzg6n95V",
        "outputId": "6ee9e590-9d2f-4b93-e525-9f75d17e1988"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1024, 51, 256, 358, 460, 512, 563, 665, 768, 1024]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-6-eea82c5d37d3>:69: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  return torch.tensor(times_tiled, device = device, dtype = dtype, requires_grad = grad_bool)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1] loss: 6.494E+01\n",
            "[2] loss: 6.322E+01\n",
            "[3] loss: 6.150E+01\n",
            "[4] loss: 5.980E+01\n",
            "[5] loss: 5.811E+01\n",
            "[6] loss: 5.645E+01\n",
            "[7] loss: 5.481E+01\n",
            "[8] loss: 5.321E+01\n",
            "[9] loss: 5.164E+01\n",
            "[10] loss: 5.011E+01\n",
            "[11] loss: 4.862E+01\n",
            "[12] loss: 4.717E+01\n",
            "[13] loss: 4.577E+01\n",
            "[14] loss: 4.441E+01\n",
            "[15] loss: 4.309E+01\n",
            "[16] loss: 4.181E+01\n",
            "[17] loss: 4.058E+01\n",
            "[18] loss: 3.939E+01\n",
            "[19] loss: 3.823E+01\n",
            "[20] loss: 3.711E+01\n",
            "[21] loss: 3.603E+01\n",
            "[22] loss: 3.498E+01\n",
            "[23] loss: 3.397E+01\n",
            "[24] loss: 3.298E+01\n",
            "[25] loss: 3.202E+01\n",
            "[26] loss: 3.110E+01\n",
            "[27] loss: 3.019E+01\n",
            "[28] loss: 2.932E+01\n",
            "[29] loss: 2.847E+01\n",
            "[30] loss: 2.764E+01\n",
            "[31] loss: 2.684E+01\n",
            "[32] loss: 2.606E+01\n",
            "[33] loss: 2.530E+01\n",
            "[34] loss: 2.457E+01\n",
            "[35] loss: 2.385E+01\n",
            "[36] loss: 2.316E+01\n",
            "[37] loss: 2.248E+01\n",
            "[38] loss: 2.182E+01\n",
            "[39] loss: 2.118E+01\n",
            "[40] loss: 2.056E+01\n",
            "[41] loss: 1.996E+01\n",
            "[42] loss: 1.938E+01\n",
            "[43] loss: 1.881E+01\n",
            "[44] loss: 1.826E+01\n",
            "[45] loss: 1.772E+01\n",
            "[46] loss: 1.720E+01\n",
            "[47] loss: 1.669E+01\n",
            "[48] loss: 1.620E+01\n",
            "[49] loss: 1.573E+01\n",
            "[50] loss: 1.526E+01\n",
            "[51] loss: 1.481E+01\n",
            "[52] loss: 1.438E+01\n",
            "[53] loss: 1.396E+01\n",
            "[54] loss: 1.355E+01\n",
            "[55] loss: 1.315E+01\n",
            "[56] loss: 1.277E+01\n",
            "[57] loss: 1.240E+01\n",
            "[58] loss: 1.204E+01\n",
            "[59] loss: 1.169E+01\n",
            "[60] loss: 1.135E+01\n",
            "[61] loss: 1.102E+01\n",
            "[62] loss: 1.070E+01\n",
            "[63] loss: 1.040E+01\n",
            "[64] loss: 1.010E+01\n",
            "[65] loss: 9.811E+00\n",
            "[66] loss: 9.531E+00\n",
            "[67] loss: 9.260E+00\n",
            "[68] loss: 8.998E+00\n",
            "[69] loss: 8.743E+00\n",
            "[70] loss: 8.496E+00\n",
            "[71] loss: 8.257E+00\n",
            "[72] loss: 8.025E+00\n",
            "[73] loss: 7.800E+00\n",
            "[74] loss: 7.581E+00\n",
            "[75] loss: 7.370E+00\n",
            "[76] loss: 7.165E+00\n",
            "[77] loss: 6.966E+00\n",
            "[78] loss: 6.773E+00\n",
            "[79] loss: 6.586E+00\n",
            "[80] loss: 6.404E+00\n",
            "[81] loss: 6.229E+00\n",
            "[82] loss: 6.058E+00\n",
            "[83] loss: 5.893E+00\n",
            "[84] loss: 5.733E+00\n",
            "[85] loss: 5.579E+00\n",
            "[86] loss: 5.429E+00\n",
            "[87] loss: 5.284E+00\n",
            "[88] loss: 5.143E+00\n",
            "[89] loss: 5.007E+00\n",
            "[90] loss: 4.876E+00\n",
            "[91] loss: 4.748E+00\n",
            "[92] loss: 4.625E+00\n",
            "[93] loss: 4.506E+00\n",
            "[94] loss: 4.391E+00\n",
            "[95] loss: 4.279E+00\n",
            "[96] loss: 4.171E+00\n",
            "[97] loss: 4.067E+00\n",
            "[98] loss: 3.966E+00\n",
            "[99] loss: 3.868E+00\n",
            "[100] loss: 3.773E+00\n",
            "[101] loss: 3.682E+00\n",
            "[102] loss: 3.593E+00\n",
            "[103] loss: 3.508E+00\n",
            "[104] loss: 3.425E+00\n",
            "[105] loss: 3.345E+00\n",
            "[106] loss: 3.267E+00\n",
            "[107] loss: 3.192E+00\n",
            "[108] loss: 3.119E+00\n",
            "[109] loss: 3.048E+00\n",
            "[110] loss: 2.980E+00\n",
            "[111] loss: 2.913E+00\n",
            "[112] loss: 2.849E+00\n",
            "[113] loss: 2.787E+00\n",
            "[114] loss: 2.727E+00\n",
            "[115] loss: 2.668E+00\n",
            "[116] loss: 2.611E+00\n",
            "[117] loss: 2.556E+00\n",
            "[118] loss: 2.503E+00\n",
            "[119] loss: 2.451E+00\n",
            "[120] loss: 2.400E+00\n",
            "[121] loss: 2.351E+00\n",
            "[122] loss: 2.304E+00\n",
            "[123] loss: 2.258E+00\n",
            "[124] loss: 2.213E+00\n",
            "[125] loss: 2.169E+00\n",
            "[126] loss: 2.127E+00\n",
            "[127] loss: 2.086E+00\n",
            "[128] loss: 2.046E+00\n",
            "[129] loss: 2.007E+00\n",
            "[130] loss: 1.969E+00\n",
            "[131] loss: 1.932E+00\n",
            "[132] loss: 1.896E+00\n",
            "[133] loss: 1.862E+00\n",
            "[134] loss: 1.828E+00\n",
            "[135] loss: 1.795E+00\n",
            "[136] loss: 1.762E+00\n",
            "[137] loss: 1.731E+00\n",
            "[138] loss: 1.701E+00\n",
            "[139] loss: 1.671E+00\n",
            "[140] loss: 1.642E+00\n",
            "[141] loss: 1.614E+00\n",
            "[142] loss: 1.586E+00\n",
            "[143] loss: 1.559E+00\n",
            "[144] loss: 1.533E+00\n",
            "[145] loss: 1.508E+00\n",
            "[146] loss: 1.483E+00\n",
            "[147] loss: 1.459E+00\n",
            "[148] loss: 1.435E+00\n",
            "[149] loss: 1.412E+00\n",
            "[150] loss: 1.389E+00\n",
            "[151] loss: 1.367E+00\n",
            "[152] loss: 1.346E+00\n",
            "[153] loss: 1.325E+00\n",
            "[154] loss: 1.305E+00\n",
            "[155] loss: 1.285E+00\n",
            "[156] loss: 1.265E+00\n",
            "[157] loss: 1.246E+00\n",
            "[158] loss: 1.228E+00\n",
            "[159] loss: 1.210E+00\n",
            "[160] loss: 1.192E+00\n",
            "[161] loss: 1.175E+00\n",
            "[162] loss: 1.158E+00\n",
            "[163] loss: 1.141E+00\n",
            "[164] loss: 1.125E+00\n",
            "[165] loss: 1.109E+00\n",
            "[166] loss: 1.094E+00\n",
            "[167] loss: 1.079E+00\n",
            "[168] loss: 1.064E+00\n",
            "[169] loss: 1.050E+00\n",
            "[170] loss: 1.036E+00\n",
            "[171] loss: 1.022E+00\n",
            "[172] loss: 1.008E+00\n",
            "[173] loss: 9.952E-01\n",
            "[174] loss: 9.823E-01\n",
            "[175] loss: 9.697E-01\n",
            "[176] loss: 9.573E-01\n",
            "[177] loss: 9.452E-01\n",
            "[178] loss: 9.334E-01\n",
            "[179] loss: 9.218E-01\n",
            "[180] loss: 9.105E-01\n",
            "[181] loss: 8.994E-01\n",
            "[182] loss: 8.886E-01\n",
            "[183] loss: 8.779E-01\n",
            "[184] loss: 8.675E-01\n",
            "[185] loss: 8.574E-01\n",
            "[186] loss: 8.474E-01\n",
            "[187] loss: 8.376E-01\n",
            "[188] loss: 8.280E-01\n",
            "[189] loss: 8.187E-01\n",
            "[190] loss: 8.095E-01\n",
            "[191] loss: 8.005E-01\n",
            "[192] loss: 7.917E-01\n",
            "[193] loss: 7.830E-01\n",
            "[194] loss: 7.746E-01\n",
            "[195] loss: 7.663E-01\n",
            "[196] loss: 7.581E-01\n",
            "[197] loss: 7.502E-01\n",
            "[198] loss: 7.424E-01\n",
            "[199] loss: 7.347E-01\n",
            "[200] loss: 7.272E-01\n",
            "[201] loss: 7.198E-01\n",
            "[202] loss: 7.126E-01\n",
            "[203] loss: 7.055E-01\n",
            "[204] loss: 6.986E-01\n",
            "[205] loss: 6.918E-01\n",
            "[206] loss: 6.851E-01\n",
            "[207] loss: 6.786E-01\n",
            "[208] loss: 6.722E-01\n",
            "[209] loss: 6.659E-01\n",
            "[210] loss: 6.597E-01\n",
            "[211] loss: 6.536E-01\n",
            "[212] loss: 6.476E-01\n",
            "[213] loss: 6.418E-01\n",
            "[214] loss: 6.361E-01\n",
            "[215] loss: 6.304E-01\n",
            "[216] loss: 6.249E-01\n",
            "[217] loss: 6.195E-01\n",
            "[218] loss: 6.142E-01\n",
            "[219] loss: 6.090E-01\n",
            "[220] loss: 6.038E-01\n",
            "[221] loss: 5.988E-01\n",
            "[222] loss: 5.939E-01\n",
            "[223] loss: 5.890E-01\n",
            "[224] loss: 5.842E-01\n",
            "[225] loss: 5.795E-01\n",
            "[226] loss: 5.749E-01\n",
            "[227] loss: 5.704E-01\n",
            "[228] loss: 5.660E-01\n",
            "[229] loss: 5.616E-01\n",
            "[230] loss: 5.574E-01\n",
            "[231] loss: 5.531E-01\n",
            "[232] loss: 5.490E-01\n",
            "[233] loss: 5.449E-01\n",
            "[234] loss: 5.410E-01\n",
            "[235] loss: 5.370E-01\n",
            "[236] loss: 5.332E-01\n",
            "[237] loss: 5.294E-01\n",
            "[238] loss: 5.257E-01\n",
            "[239] loss: 5.220E-01\n",
            "[240] loss: 5.184E-01\n",
            "[241] loss: 5.149E-01\n",
            "[242] loss: 5.114E-01\n",
            "[243] loss: 5.080E-01\n",
            "[244] loss: 5.046E-01\n",
            "[245] loss: 5.013E-01\n",
            "[246] loss: 4.981E-01\n",
            "[247] loss: 4.949E-01\n",
            "[248] loss: 4.917E-01\n",
            "[249] loss: 4.886E-01\n",
            "[250] loss: 4.856E-01\n",
            "[251] loss: 4.826E-01\n",
            "[252] loss: 4.796E-01\n",
            "[253] loss: 4.767E-01\n",
            "[254] loss: 4.739E-01\n",
            "[255] loss: 4.711E-01\n",
            "[256] loss: 4.683E-01\n",
            "[257] loss: 4.656E-01\n",
            "[258] loss: 4.630E-01\n",
            "[259] loss: 4.603E-01\n",
            "[260] loss: 4.577E-01\n",
            "[261] loss: 4.552E-01\n",
            "[262] loss: 4.527E-01\n",
            "[263] loss: 4.502E-01\n",
            "[264] loss: 4.478E-01\n",
            "[265] loss: 4.454E-01\n",
            "[266] loss: 4.430E-01\n",
            "[267] loss: 4.407E-01\n",
            "[268] loss: 4.385E-01\n",
            "[269] loss: 4.362E-01\n",
            "[270] loss: 4.340E-01\n",
            "[271] loss: 4.318E-01\n",
            "[272] loss: 4.297E-01\n",
            "[273] loss: 4.276E-01\n",
            "[274] loss: 4.255E-01\n",
            "[275] loss: 4.234E-01\n",
            "[276] loss: 4.214E-01\n",
            "[277] loss: 4.194E-01\n",
            "[278] loss: 4.175E-01\n",
            "[279] loss: 4.155E-01\n",
            "[280] loss: 4.136E-01\n",
            "[281] loss: 4.118E-01\n",
            "[282] loss: 4.099E-01\n",
            "[283] loss: 4.081E-01\n",
            "[284] loss: 4.063E-01\n",
            "[285] loss: 4.045E-01\n",
            "[286] loss: 4.028E-01\n",
            "[287] loss: 4.011E-01\n",
            "[288] loss: 3.994E-01\n",
            "[289] loss: 3.977E-01\n",
            "[290] loss: 3.961E-01\n",
            "[291] loss: 3.945E-01\n",
            "[292] loss: 3.929E-01\n",
            "[293] loss: 3.913E-01\n",
            "[294] loss: 3.898E-01\n",
            "[295] loss: 3.882E-01\n",
            "[296] loss: 3.867E-01\n",
            "[297] loss: 3.852E-01\n",
            "[298] loss: 3.838E-01\n",
            "[299] loss: 3.823E-01\n",
            "[300] loss: 3.809E-01\n",
            "[301] loss: 3.795E-01\n",
            "[302] loss: 3.781E-01\n",
            "[303] loss: 3.767E-01\n",
            "[304] loss: 3.754E-01\n",
            "[305] loss: 3.741E-01\n",
            "[306] loss: 3.727E-01\n",
            "[307] loss: 3.714E-01\n",
            "[308] loss: 3.702E-01\n",
            "[309] loss: 3.689E-01\n",
            "[310] loss: 3.677E-01\n",
            "[311] loss: 3.664E-01\n",
            "[312] loss: 3.652E-01\n",
            "[313] loss: 3.640E-01\n",
            "[314] loss: 3.628E-01\n",
            "[315] loss: 3.617E-01\n",
            "[316] loss: 3.605E-01\n",
            "[317] loss: 3.594E-01\n",
            "[318] loss: 3.583E-01\n",
            "[319] loss: 3.572E-01\n",
            "[320] loss: 3.561E-01\n",
            "[321] loss: 3.550E-01\n",
            "[322] loss: 3.540E-01\n",
            "[323] loss: 3.529E-01\n",
            "[324] loss: 3.519E-01\n",
            "[325] loss: 3.508E-01\n",
            "[326] loss: 3.498E-01\n",
            "[327] loss: 3.488E-01\n",
            "[328] loss: 3.479E-01\n",
            "[329] loss: 3.469E-01\n",
            "[330] loss: 3.459E-01\n",
            "[331] loss: 3.450E-01\n",
            "[332] loss: 3.440E-01\n",
            "[333] loss: 3.431E-01\n",
            "[334] loss: 3.422E-01\n",
            "[335] loss: 3.413E-01\n",
            "[336] loss: 3.404E-01\n",
            "[337] loss: 3.395E-01\n",
            "[338] loss: 3.387E-01\n",
            "[339] loss: 3.378E-01\n",
            "[340] loss: 3.370E-01\n",
            "[341] loss: 3.361E-01\n",
            "[342] loss: 3.353E-01\n",
            "[343] loss: 3.345E-01\n",
            "[344] loss: 3.337E-01\n",
            "[345] loss: 3.329E-01\n",
            "[346] loss: 3.321E-01\n",
            "[347] loss: 3.313E-01\n",
            "[348] loss: 3.305E-01\n",
            "[349] loss: 3.298E-01\n",
            "[350] loss: 3.290E-01\n",
            "[351] loss: 3.283E-01\n",
            "[352] loss: 3.275E-01\n",
            "[353] loss: 3.268E-01\n",
            "[354] loss: 3.261E-01\n",
            "[355] loss: 3.254E-01\n",
            "[356] loss: 3.247E-01\n",
            "[357] loss: 3.240E-01\n",
            "[358] loss: 3.233E-01\n",
            "[359] loss: 3.226E-01\n",
            "[360] loss: 3.219E-01\n",
            "[361] loss: 3.213E-01\n",
            "[362] loss: 3.206E-01\n",
            "[363] loss: 3.199E-01\n",
            "[364] loss: 3.193E-01\n",
            "[365] loss: 3.187E-01\n",
            "[366] loss: 3.180E-01\n",
            "[367] loss: 3.174E-01\n",
            "[368] loss: 3.168E-01\n",
            "[369] loss: 3.162E-01\n",
            "[370] loss: 3.156E-01\n",
            "[371] loss: 3.150E-01\n",
            "[372] loss: 3.144E-01\n",
            "[373] loss: 3.138E-01\n",
            "[374] loss: 3.132E-01\n",
            "[375] loss: 3.127E-01\n",
            "[376] loss: 3.121E-01\n",
            "[377] loss: 3.115E-01\n",
            "[378] loss: 3.110E-01\n",
            "[379] loss: 3.104E-01\n",
            "[380] loss: 3.099E-01\n",
            "[381] loss: 3.094E-01\n",
            "[382] loss: 3.088E-01\n",
            "[383] loss: 3.083E-01\n",
            "[384] loss: 3.078E-01\n",
            "[385] loss: 3.073E-01\n",
            "[386] loss: 3.068E-01\n",
            "[387] loss: 3.063E-01\n",
            "[388] loss: 3.058E-01\n",
            "[389] loss: 3.053E-01\n",
            "[390] loss: 3.048E-01\n",
            "[391] loss: 3.043E-01\n",
            "[392] loss: 3.038E-01\n",
            "[393] loss: 3.033E-01\n",
            "[394] loss: 3.029E-01\n",
            "[395] loss: 3.024E-01\n",
            "[396] loss: 3.019E-01\n",
            "[397] loss: 3.015E-01\n",
            "[398] loss: 3.010E-01\n",
            "[399] loss: 3.006E-01\n",
            "[400] loss: 3.001E-01\n",
            "[401] loss: 2.997E-01\n",
            "[402] loss: 2.993E-01\n",
            "[403] loss: 2.988E-01\n",
            "[404] loss: 2.984E-01\n",
            "[405] loss: 2.980E-01\n",
            "[406] loss: 2.976E-01\n",
            "[407] loss: 2.972E-01\n",
            "[408] loss: 2.967E-01\n",
            "[409] loss: 2.963E-01\n",
            "[410] loss: 2.959E-01\n",
            "[411] loss: 2.955E-01\n",
            "[412] loss: 2.951E-01\n",
            "[413] loss: 2.947E-01\n",
            "[414] loss: 2.944E-01\n",
            "[415] loss: 2.940E-01\n",
            "[416] loss: 2.936E-01\n",
            "[417] loss: 2.932E-01\n",
            "[418] loss: 2.928E-01\n",
            "[419] loss: 2.925E-01\n",
            "[420] loss: 2.921E-01\n",
            "[421] loss: 2.917E-01\n",
            "[422] loss: 2.914E-01\n",
            "[423] loss: 2.910E-01\n",
            "[424] loss: 2.906E-01\n",
            "[425] loss: 2.903E-01\n",
            "[426] loss: 2.899E-01\n",
            "[427] loss: 2.896E-01\n",
            "[428] loss: 2.892E-01\n",
            "[429] loss: 2.889E-01\n",
            "[430] loss: 2.886E-01\n",
            "[431] loss: 2.882E-01\n",
            "[432] loss: 2.879E-01\n",
            "[433] loss: 2.876E-01\n",
            "[434] loss: 2.872E-01\n",
            "[435] loss: 2.869E-01\n",
            "[436] loss: 2.866E-01\n",
            "[437] loss: 2.863E-01\n",
            "[438] loss: 2.859E-01\n",
            "[439] loss: 2.856E-01\n",
            "[440] loss: 2.853E-01\n",
            "[441] loss: 2.850E-01\n",
            "[442] loss: 2.847E-01\n",
            "[443] loss: 2.844E-01\n",
            "[444] loss: 2.841E-01\n",
            "[445] loss: 2.838E-01\n",
            "[446] loss: 2.835E-01\n",
            "[447] loss: 2.832E-01\n",
            "[448] loss: 2.829E-01\n",
            "[449] loss: 2.826E-01\n",
            "[450] loss: 2.823E-01\n",
            "[451] loss: 2.820E-01\n",
            "[452] loss: 2.818E-01\n",
            "[453] loss: 2.815E-01\n",
            "[454] loss: 2.812E-01\n",
            "[455] loss: 2.809E-01\n",
            "[456] loss: 2.806E-01\n",
            "[457] loss: 2.804E-01\n",
            "[458] loss: 2.801E-01\n",
            "[459] loss: 2.798E-01\n",
            "[460] loss: 2.796E-01\n",
            "[461] loss: 2.793E-01\n",
            "[462] loss: 2.790E-01\n",
            "[463] loss: 2.788E-01\n",
            "[464] loss: 2.785E-01\n",
            "[465] loss: 2.782E-01\n",
            "[466] loss: 2.780E-01\n",
            "[467] loss: 2.777E-01\n",
            "[468] loss: 2.775E-01\n",
            "[469] loss: 2.772E-01\n",
            "[470] loss: 2.770E-01\n",
            "[471] loss: 2.767E-01\n",
            "[472] loss: 2.765E-01\n",
            "[473] loss: 2.762E-01\n",
            "[474] loss: 2.760E-01\n",
            "[475] loss: 2.757E-01\n",
            "[476] loss: 2.755E-01\n",
            "[477] loss: 2.753E-01\n",
            "[478] loss: 2.750E-01\n",
            "[479] loss: 2.748E-01\n",
            "[480] loss: 2.746E-01\n",
            "[481] loss: 2.743E-01\n",
            "[482] loss: 2.741E-01\n",
            "[483] loss: 2.739E-01\n",
            "[484] loss: 2.736E-01\n",
            "[485] loss: 2.734E-01\n",
            "[486] loss: 2.732E-01\n",
            "[487] loss: 2.730E-01\n",
            "[488] loss: 2.727E-01\n",
            "[489] loss: 2.725E-01\n",
            "[490] loss: 2.723E-01\n",
            "[491] loss: 2.721E-01\n",
            "[492] loss: 2.719E-01\n",
            "[493] loss: 2.716E-01\n",
            "[494] loss: 2.714E-01\n",
            "[495] loss: 2.712E-01\n",
            "[496] loss: 2.710E-01\n",
            "[497] loss: 2.708E-01\n",
            "[498] loss: 2.706E-01\n",
            "[499] loss: 2.704E-01\n",
            "[500] loss: 2.702E-01\n",
            "[501] loss: 2.700E-01\n",
            "[502] loss: 2.698E-01\n",
            "[503] loss: 2.696E-01\n",
            "[504] loss: 2.694E-01\n",
            "[505] loss: 2.692E-01\n",
            "[506] loss: 2.690E-01\n",
            "[507] loss: 2.688E-01\n",
            "[508] loss: 2.686E-01\n",
            "[509] loss: 2.684E-01\n",
            "[510] loss: 2.682E-01\n",
            "[511] loss: 2.680E-01\n",
            "[512] loss: 2.678E-01\n",
            "[513] loss: 2.676E-01\n",
            "[514] loss: 2.674E-01\n",
            "[515] loss: 2.672E-01\n",
            "[516] loss: 2.670E-01\n",
            "[517] loss: 2.668E-01\n",
            "[518] loss: 2.666E-01\n",
            "[519] loss: 2.664E-01\n",
            "[520] loss: 2.663E-01\n",
            "[521] loss: 2.661E-01\n",
            "[522] loss: 2.659E-01\n",
            "[523] loss: 2.657E-01\n",
            "[524] loss: 2.655E-01\n",
            "[525] loss: 2.653E-01\n",
            "[526] loss: 2.652E-01\n",
            "[527] loss: 2.650E-01\n",
            "[528] loss: 2.648E-01\n",
            "[529] loss: 2.646E-01\n",
            "[530] loss: 2.645E-01\n",
            "[531] loss: 2.643E-01\n",
            "[532] loss: 2.641E-01\n",
            "[533] loss: 2.639E-01\n",
            "[534] loss: 2.638E-01\n",
            "[535] loss: 2.636E-01\n",
            "[536] loss: 2.634E-01\n",
            "[537] loss: 2.632E-01\n",
            "[538] loss: 2.631E-01\n",
            "[539] loss: 2.629E-01\n",
            "[540] loss: 2.627E-01\n",
            "[541] loss: 2.626E-01\n",
            "[542] loss: 2.624E-01\n",
            "[543] loss: 2.622E-01\n",
            "[544] loss: 2.621E-01\n",
            "[545] loss: 2.619E-01\n",
            "[546] loss: 2.617E-01\n",
            "[547] loss: 2.616E-01\n",
            "[548] loss: 2.614E-01\n",
            "[549] loss: 2.612E-01\n",
            "[550] loss: 2.611E-01\n",
            "[551] loss: 2.609E-01\n",
            "[552] loss: 2.608E-01\n",
            "[553] loss: 2.606E-01\n",
            "[554] loss: 2.604E-01\n",
            "[555] loss: 2.603E-01\n",
            "[556] loss: 2.601E-01\n",
            "[557] loss: 2.600E-01\n",
            "[558] loss: 2.598E-01\n",
            "[559] loss: 2.596E-01\n",
            "[560] loss: 2.595E-01\n",
            "[561] loss: 2.593E-01\n",
            "[562] loss: 2.592E-01\n",
            "[563] loss: 2.590E-01\n",
            "[564] loss: 2.589E-01\n",
            "[565] loss: 2.587E-01\n",
            "[566] loss: 2.586E-01\n",
            "[567] loss: 2.584E-01\n",
            "[568] loss: 2.583E-01\n",
            "[569] loss: 2.581E-01\n",
            "[570] loss: 2.580E-01\n",
            "[571] loss: 2.578E-01\n",
            "[572] loss: 2.577E-01\n",
            "[573] loss: 2.575E-01\n",
            "[574] loss: 2.574E-01\n",
            "[575] loss: 2.572E-01\n",
            "[576] loss: 2.571E-01\n",
            "[577] loss: 2.569E-01\n",
            "[578] loss: 2.568E-01\n",
            "[579] loss: 2.566E-01\n",
            "[580] loss: 2.565E-01\n",
            "[581] loss: 2.564E-01\n",
            "[582] loss: 2.562E-01\n",
            "[583] loss: 2.561E-01\n",
            "[584] loss: 2.559E-01\n",
            "[585] loss: 2.558E-01\n",
            "[586] loss: 2.556E-01\n",
            "[587] loss: 2.555E-01\n",
            "[588] loss: 2.554E-01\n",
            "[589] loss: 2.552E-01\n",
            "[590] loss: 2.551E-01\n",
            "[591] loss: 2.549E-01\n",
            "[592] loss: 2.548E-01\n",
            "[593] loss: 2.547E-01\n",
            "[594] loss: 2.545E-01\n",
            "[595] loss: 2.544E-01\n",
            "[596] loss: 2.543E-01\n",
            "[597] loss: 2.541E-01\n",
            "[598] loss: 2.540E-01\n",
            "[599] loss: 2.538E-01\n",
            "[600] loss: 2.537E-01\n",
            "[601] loss: 2.536E-01\n",
            "[602] loss: 2.534E-01\n",
            "[603] loss: 2.533E-01\n",
            "[604] loss: 2.532E-01\n",
            "[605] loss: 2.530E-01\n",
            "[606] loss: 2.529E-01\n",
            "[607] loss: 2.528E-01\n",
            "[608] loss: 2.526E-01\n",
            "[609] loss: 2.525E-01\n",
            "[610] loss: 2.524E-01\n",
            "[611] loss: 2.522E-01\n",
            "[612] loss: 2.521E-01\n",
            "[613] loss: 2.520E-01\n",
            "[614] loss: 2.518E-01\n",
            "[615] loss: 2.517E-01\n",
            "[616] loss: 2.516E-01\n",
            "[617] loss: 2.515E-01\n",
            "[618] loss: 2.513E-01\n",
            "[619] loss: 2.512E-01\n",
            "[620] loss: 2.511E-01\n",
            "[621] loss: 2.509E-01\n",
            "[622] loss: 2.508E-01\n",
            "[623] loss: 2.507E-01\n",
            "[624] loss: 2.506E-01\n",
            "[625] loss: 2.504E-01\n",
            "[626] loss: 2.503E-01\n",
            "[627] loss: 2.502E-01\n",
            "[628] loss: 2.501E-01\n",
            "[629] loss: 2.499E-01\n",
            "[630] loss: 2.498E-01\n",
            "[631] loss: 2.497E-01\n",
            "[632] loss: 2.496E-01\n",
            "[633] loss: 2.494E-01\n",
            "[634] loss: 2.493E-01\n",
            "[635] loss: 2.492E-01\n",
            "[636] loss: 2.491E-01\n",
            "[637] loss: 2.489E-01\n",
            "[638] loss: 2.488E-01\n",
            "[639] loss: 2.487E-01\n",
            "[640] loss: 2.486E-01\n",
            "[641] loss: 2.484E-01\n",
            "[642] loss: 2.483E-01\n",
            "[643] loss: 2.482E-01\n",
            "[644] loss: 2.481E-01\n",
            "[645] loss: 2.480E-01\n",
            "[646] loss: 2.478E-01\n",
            "[647] loss: 2.477E-01\n",
            "[648] loss: 2.476E-01\n",
            "[649] loss: 2.475E-01\n",
            "[650] loss: 2.474E-01\n",
            "[651] loss: 2.472E-01\n",
            "[652] loss: 2.471E-01\n",
            "[653] loss: 2.470E-01\n",
            "[654] loss: 2.469E-01\n",
            "[655] loss: 2.468E-01\n",
            "[656] loss: 2.466E-01\n",
            "[657] loss: 2.465E-01\n",
            "[658] loss: 2.464E-01\n",
            "[659] loss: 2.463E-01\n",
            "[660] loss: 2.462E-01\n",
            "[661] loss: 2.460E-01\n"
          ]
        }
      ],
      "source": [
        "from torch.autograd import Variable\n",
        "import numpy as np\n",
        "import math\n",
        "from scipy.stats import unitary_group\n",
        "\n",
        "\n",
        "\n",
        "# default grad_enabled\n",
        "torch.set_grad_enabled(grad_enabled_bool)\n",
        "\n",
        "n = 32\t\t\t# dimension\n",
        "lr = 0.0001\t\t# learning rate for gd\n",
        "n_repeats = 10  # number of time to randomly repeat for given set of parameters\n",
        "\n",
        "save_results = True \t\t# set to True to save outputs as csv files\n",
        "print_freq = 1\t\t\t\t# prints to csv file at every print_freq steps\n",
        "\n",
        "# list of dimensions for target unitary relative to number of parameters\n",
        "dimensions_raw = [2.0, 0.1, 0.5, 0.7, 0.9, 1.0, 1.1, 1.3, 1.5, 2.0]\n",
        "\n",
        "dimensions = [int(p*n*n/2.) for p in dimensions_raw]\n",
        "print(dimensions)\n",
        "\n",
        "# list of parameters in target unitary (set to None to have Haar Random)\n",
        "n_targets = [None]\t\t\t\t\t# number of parameters in target unitary (set to None to have Haar Random)\n",
        "# n_targets = [int(n*n)]\t\t\t# number of parameters in target unitary (set to None to have Haar Random)\n",
        "\n",
        "\n",
        "\n",
        "for i, ii in zip(dimensions, dimensions_raw):\n",
        "\n",
        "\n",
        "\tfor j in n_targets:\n",
        "\t\tfor k in range(n_repeats):\n",
        "\n",
        "\t\t\tA = create_GUE(n)\t# unitary 1\n",
        "\t\t\tB = create_GUE(n)\t# unitary 2\n",
        "\t\t\trandom_A_B = torch.zeros( (2,2,n,n), dtype = dtype, device = device )\t\t#formatting\n",
        "\t\t\trandom_A_B[0,0,:,:] = torch.tensor(A.real, dtype = dtype, device = device )\t#formatting\n",
        "\t\t\trandom_A_B[0,1,:,:] = torch.tensor(A.imag, dtype = dtype, device = device )\t#formatting\n",
        "\t\t\trandom_A_B[1,0,:,:] = torch.tensor(B.real, dtype = dtype, device = device )\t#formatting\n",
        "\t\t\trandom_A_B[1,1,:,:] = torch.tensor(B.imag, dtype = dtype, device = device )\t#formatting\n",
        "\n",
        "\t\t\t# if number of targets is set to None, then we select a random Haar unitary as target\n",
        "\t\t\tif j is None:\n",
        "\t\t\t\ttarget_unitary = unitary_group.rvs(n)\n",
        "\t\t\t\ttarget_formatted = torch.zeros( (1,2,n,n), dtype = dtype, device = device )\t\t#formatting\n",
        "\t\t\t\ttarget_formatted[0,0,:,:] = torch.tensor(target_unitary.real, dtype = dtype, device = device )\t#formatting\n",
        "\t\t\t\ttarget_formatted[0,1,:,:] = torch.tensor(target_unitary.imag, dtype = dtype, device = device )\t#formatting\n",
        "\t\t\t\ta = unitary_optimizer(control_matrices = random_A_B, target = target_formatted )\n",
        "\t\t\telse:\n",
        "\t\t\t\ta = unitary_optimizer(control_matrices = random_A_B, random_time_target = j )\n",
        "\n",
        "\n",
        "\n",
        "\t\t\t# example setup for vanilla gradient descent optimizer\n",
        "\t\t\ta.optimize(n_epochs = 10000, lr = lr/ii, weight_decay = 0.0, manual_grad_calc = True,\n",
        "\t\t\t\t\t\tn_times = i, optim_type = 'SGD', save_results = save_results, print_every = print_freq,\n",
        "\t\t\t\t\t\tabsolute_stop_rate = 1e-7*n*n, momentum = 0 ) # n_times = n_layers\n",
        "\n",
        "\t\t\t# example setup for adam optimizer\n",
        "\t\t\t# a.optimize(n_epochs = 10000, lr = 0.001/ii, weight_decay = 0.0, manual_grad_calc = True,\n",
        "\t\t\t# \t\t\tn_times = i, optim_type = 'Adam', save_results = save_results, print_every = print_freq,\n",
        "\t\t\t# \t\t\tabsolute_stop_rate = 1e-7*n*n, amsgrad = True ) # n_times = n_layers"
      ]
    }
  ]
}